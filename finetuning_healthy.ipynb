{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook performs fine-tuning of gpt-3.5-turbo to set the style of response when asked whether a habit is healthy or unhealthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install numpy\n",
    "!pip install tiktoken\n",
    "!pip install Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"your openai key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dataset of 93 healthy and unhealthy human habits is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'habits_dataset.csv'\n",
    "cleaned_data = []\n",
    "\n",
    "with open(csv_file, 'r', encoding='utf-8-sig') as file:\n",
    "  csv_reader = csv.reader(file)\n",
    "  for row in csv_reader:\n",
    "    for cell in row:\n",
    "      try:\n",
    "        # Replace square brackets and inner double quotes that are problematic\n",
    "                cell = cell.replace('[\"', '').replace('\"]', '').replace('\\\\\"', '\"')\n",
    "\n",
    "                # Load each cell as a JSON object\n",
    "                cell_json = json.loads(cell)\n",
    "\n",
    "                # Now that the content is clean, append to cleaned_data list\n",
    "                cleaned_data.append(cell_json)\n",
    "\n",
    "      except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode error for cell '{cell}': {e}\")\n",
    "\n",
    "jsonl_file_path = 'habits_dataset_json.jsonl'\n",
    "\n",
    "# Write cleaned data to a JSONL file\n",
    "with open(jsonl_file_path, 'w', encoding='utf-8') as jsonl_file:\n",
    "    for item in cleaned_data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 93\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a very helpful assistant who helps people understand what is healthy and what is unhealthy'}\n",
      "{'role': 'user', 'content': 'I drink very less water'}\n",
      "{'role': 'assistant', 'content': 'Unhealthy'}\n",
      "No errors found\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 36, 47\n",
      "mean / median: 40.61290322580645, 41.0\n",
      "p5 / p95: 38.0, 44.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 1, 9\n",
      "mean / median: 1.881720430107527, 1.0\n",
      "p5 / p95: 1.0, 2.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n",
      "Dataset has ~3777 tokens that will be charged for during training\n",
      "By default, you'll train for 3 epochs on this dataset\n",
      "By default, you'll be charged for ~11331 tokens\n",
      "Estimated cost for fine-tuning: approximately $0.09\n"
     ]
    }
   ],
   "source": [
    "#from OpenAI website to format data;  https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset\n",
    "\n",
    "# Next, we specify the data path and open the JSONL file\n",
    "\n",
    "data_path = 'habits_dataset_json.jsonl'\n",
    "\n",
    "# Load dataset\n",
    "with open(data_path) as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# We can inspect the data quickly by checking the number of examples and the first item\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "\n",
    "# Now that we have a sense of the data, we need to go through all the different examples and check to make sure the formatting is correct and matches the Chat completions message structure\n",
    "\n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "\n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "\n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "\n",
    "        if any(k not in (\"role\", \"content\", \"name\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "\n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "\n",
    "        content = message.get(\"content\", None)\n",
    "        if not content or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "\n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "# Beyond the structure of the message, we also need to ensure that the length does not exceed the 4096 token limit.\n",
    "\n",
    "# Token counting functions\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "# Last, we can look at the results of the different formatting operations before proceeding with creating a fine-tuning job:\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "\n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")\n",
    "\n",
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_EPOCHS = 1\n",
    "MAX_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")\n",
    "\n",
    "# Calculate the estimated cost for fine-tuning\n",
    "cost_per_100k_tokens = 0.80  # Cost for every 100,000 tokens\n",
    "estimated_cost = ((n_epochs * n_billing_tokens_in_dataset) / 100000) * cost_per_100k_tokens\n",
    "print(f\"Estimated cost for fine-tuning: approximately ${estimated_cost:.2f}\") #I added this for actual cost based on current pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the dataset as a JSONL file\n",
    "def save_to_jsonl(conversations, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for conversation in conversations:\n",
    "            json_line = json.dumps(conversation)\n",
    "            file.write(json_line + '\\n')\n",
    "\n",
    "# Specify the path where you want to save the JSONL file in your Google Drive\n",
    "jsonl_file_path = 'habits_dataset_json_clean.jsonl'\n",
    "# Save the dataset to the specified file path\n",
    "save_to_jsonl(dataset, jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file id: file-t3HPWbFg7NfpUC0jUJecrpa6\n"
     ]
    }
   ],
   "source": [
    "# Upload data for training\n",
    "training_file_name = 'habits_dataset_json_clean.jsonl'\n",
    "\n",
    "training_response = openai.File.create(\n",
    "  file=open(training_file_name, \"rb\"),\n",
    "  purpose='fine-tune'\n",
    ")\n",
    "training_file_id = training_response[\"id\"]\n",
    "\n",
    "# Gives training file id\n",
    "print(\"Training file id:\", training_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-DUasfitoBHyZKocAdGYReuJD\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699299142,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-ab1MNYWRPG32jAUDTlkrtkwb\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-t3HPWbFg7NfpUC0jUJecrpa6\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": \"auto\"\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create Fine-Tuning Job\n",
    "suffix_name = \"chatner-bot\"\n",
    "\n",
    "response = openai.FineTuningJob.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    suffix=suffix_name,\n",
    ")\n",
    "\n",
    "job_id = response[\"id\"]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fine-tuning job: ftjob-DUasfitoBHyZKocAdGYReuJD\n",
      "Validating training file: file-t3HPWbFg7NfpUC0jUJecrpa6\n",
      "Files validated, moving job to queued state\n",
      "Fine-tuning job started\n",
      "Step 1/279: training loss=13.30\n",
      "Step 11/279: training loss=4.89\n",
      "Step 21/279: training loss=0.04\n",
      "Step 31/279: training loss=0.00\n",
      "Step 41/279: training loss=0.00\n",
      "Step 51/279: training loss=0.00\n",
      "Step 61/279: training loss=0.00\n",
      "Step 71/279: training loss=0.00\n",
      "Step 81/279: training loss=2.91\n",
      "Step 91/279: training loss=1.82\n",
      "Step 101/279: training loss=0.00\n",
      "Step 111/279: training loss=0.00\n",
      "Step 121/279: training loss=0.00\n",
      "Step 131/279: training loss=1.11\n",
      "Step 141/279: training loss=0.00\n",
      "Step 151/279: training loss=0.00\n",
      "Step 161/279: training loss=0.00\n",
      "Step 171/279: training loss=0.18\n",
      "Step 181/279: training loss=0.00\n",
      "Step 191/279: training loss=0.00\n",
      "Step 201/279: training loss=0.00\n",
      "Step 211/279: training loss=0.00\n",
      "Step 221/279: training loss=0.00\n",
      "Step 231/279: training loss=0.00\n",
      "Step 241/279: training loss=0.00\n",
      "Step 251/279: training loss=0.00\n",
      "Step 261/279: training loss=0.00\n",
      "Step 271/279: training loss=0.00\n",
      "New fine-tuned model created: ft:gpt-3.5-turbo-0613:personal:chatner-bot:8HzoN0Ni\n",
      "The job has successfully completed\n"
     ]
    }
   ],
   "source": [
    "# list events as fine-tuning progresses\n",
    "response = openai.FineTuningJob.list_events(id=job_id, limit=50)\n",
    "\n",
    "events = response[\"data\"]\n",
    "events.reverse()\n",
    "\n",
    "for event in events:\n",
    "    print(event[\"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-DUasfitoBHyZKocAdGYReuJD\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1699299142,\n",
      "  \"finished_at\": 1699299870,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:personal:chatner-bot:8HzoN0Ni\",\n",
      "  \"organization_id\": \"org-ab1MNYWRPG32jAUDTlkrtkwb\",\n",
      "  \"result_files\": [\n",
      "    \"file-IXQwvYqBqc8o9dq10IctFLln\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": null,\n",
      "  \"training_file\": \"file-t3HPWbFg7NfpUC0jUJecrpa6\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 10773,\n",
      "  \"error\": null\n",
      "}\n",
      "\n",
      "Fine-tuned model id: ft:gpt-3.5-turbo-0613:personal:chatner-bot:8HzoN0Ni\n"
     ]
    }
   ],
   "source": [
    "# retrieve fine-tune model id\n",
    "response = openai.FineTuningJob.retrieve(job_id)\n",
    "fine_tuned_model_id = response[\"fine_tuned_model\"]\n",
    "\n",
    "print(response)\n",
    "print(\"\\nFine-tuned model id:\", fine_tuned_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a very helpful assistant who helps people understand what is healthy and what is unhealthy'}, {'role': 'user', 'content': 'Eating oats for breakfast'}]\n"
     ]
    }
   ],
   "source": [
    "test_messages = []\n",
    "\n",
    "system_message = \"You are a very helpful assistant who helps people understand what is healthy and what is unhealthy\"\n",
    "test_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "user_message = \"Eating oats for breakfast\"\n",
    "test_messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "print(test_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response from the fine-tuned gpt-3.5-turbo-0613 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    # model=fine_tuned_model_id, #can test it against gpt-3.5-turbo to see difference\n",
    "    model= \"ft:gpt-3.5-turbo-0613:personal:chatner-bot:8HzoN0Ni\", \n",
    "    messages=test_messages,\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response from gpt-3.5-turbo-0613 without fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eating oats for breakfast is a healthy choice! Oats are a whole grain that are packed with nutrients and fiber. They are a great source of complex carbohydrates, which provide sustained energy throughout the morning. Oats also contain important vitamins and minerals like iron, magnesium, and B vitamins.\n",
      "\n",
      "Additionally, oats are known for their high fiber content, which can help promote healthy digestion and keep you feeling full for longer. This can be particularly beneficial for weight management and preventing overeating later in the day.\n",
      "\n",
      "To make your oatmeal even healthier, you can add toppings like fresh fruits, nuts, or seeds for added nutrients and flavor. Just be mindful of the portion size and avoid adding excessive amounts of sugar or unhealthy toppings.\n",
      "\n",
      "Overall, starting your day with a bowl of oats is a nutritious choice that can contribute to a balanced and healthy diet.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model= \"gpt-3.5-turbo-0613\", #testing it against gpt-3.5-turbo to see difference\n",
    "    messages=test_messages,\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio for a better UI to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_completion(user_prompt):\n",
    "    system_message = \"You are a very helpful assistant who helps people understand what is healthy and what is unhealthy\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = \"ft:gpt-3.5-turbo-0613:personal:chatner-bot:8HzoN0Ni\",\n",
    "        messages = messages,\n",
    "        max_tokens = 100,\n",
    "        temperature = 0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn = generate_completion,\n",
    "    inputs = gr.Textbox(lines=5, placeholder='Type a habit to check if it is healthy or not'),\n",
    "    outputs = 'text',\n",
    "    title = \"Habit checker\"\n",
    ")\n",
    "\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
